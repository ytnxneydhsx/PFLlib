### 通用参数

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--goal` | `-go` | 字符串 | "test" | 本次实验的主要目标或名称。 |
| `--device` | `-dev` | 字符串 | "cuda" | 用于模型训练的计算设备，可选 "cpu" 或 "cuda"。 |
| `--device_id` | `-did` | 字符串 | "0" | 指定要使用的设备的具体 ID。 |
| `--dataset` | `-data` | 字符串 | "MNIST" | 用于实验的数据集名称。 |
| `--num_classes`| `-ncl`| 整数 | 10 | 数据集中的类别总数。 |
| `--model` | `-m` | 字符串 | "CNN" | 要使用的神经网络模型架构。 |
| `--batch_size` | `-lbs` | 整数 | 10 | 训练时每个批次（batch）的样本数量。 |
| `--local_learning_rate` | `-lr` | 浮点数 | 0.005 | 客户端本地更新时的学习率。 |
| `--learning_rate_decay` | `-ld` | 布尔值 | False | 如果为 True，学习率将随着训练进行而衰减。 |
| `--learning_rate_decay_gamma`| `-ldg`| 浮点数 | 0.99 | 学习率衰减的gamma系数。 |
| `--global_rounds` | `-gr` | 整数 | 2000 | 服务器和客户端之间的总通信轮次。 |
| `--top_cnt` | `-tc` | 整数 | 100 | 用于 `auto_break` 功能，表示在满足条件后需要保持的轮次数。 |
| `--local_epochs`| `-ls` | 整数 | 1 | 在单次全局通信轮次中，每个客户端执行的本地训练周期（epoch）数。 |
| `--algorithm` | `-algo` | 字符串 | "FedAvg" | 要使用的联邦学习算法。 |
| `--join_ratio` | `-jr` | 浮点数 | 1.0 | 每轮参与训练的客户端比例。 |
| `--random_join_ratio`| `-rjr`| 布尔值 | False | 如果为 True，每轮将随机选择一部分客户端参与。 |
| `--num_clients`| `-nc` | 整数 | 20 | 联邦网络中的客户端总数。 |
| `--prev` | `-pv` | 整数 | 0 | 先前已经运行的次数（用于断点续跑）。 |
| `--times` | `-t` | 整数 | 1 | 实验需要运行的总次数。 |
| `--eval_gap` | `-eg` | 整数 | 1 | 进行模型评估的间隔轮次。 |
| `--save_folder_name`| `-sfn` | 字符串 | "items" | 保存实验结果的文件夹名称。 |
| `--auto_break` | `-ab` | 布尔值 | False | 如果为 True，当满足特定条件时训练将自动停止。 |
| `--dlg_eval` | `-dlg` | 布尔值 | False | 一个标志，用于启用 DLG (Deep Leakage from Gradients) 梯度泄露攻击的评估。 |
| `--dlg_gap` | `-dlgg` | 整数 | 100 | 进行 DLG 评估的间隔轮次。 |
| `--batch_num_per_client` | `-bnpc` | 整数 | 2 | 每个客户端拥有的数据批次数。 |
| `--num_new_clients` | `-nnc` | 整数 | 0 | 训练过程中新增的客户端数量。 |
| `--fine_tuning_epoch_new` | `-ften` | 整数 | 0 | 新客户端加入后进行微调的周期数。 |
| `--feature_dim`| `-fd` | 整数 | 512 | 特征向量的维度。 |
| `--vocab_size` | `-vs` | 整数 | 80 | 词汇表大小，主要用于自然语言处理任务。 |
| `--max_len` | `-ml` | 整数 | 200 | 文本序列的最大长度，用于自然语言处理任务。 |
| `--few_shot` | `-fs` | 整数 | 0 | 小样本学习中每个类别的样本数。 |

### 模拟现实场景参数

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--client_drop_rate` | `-cdr` | 浮点数 | 0.0 | 客户端在训练过程中掉线的比率。 |
| `--train_slow_rate` | `-tsr` | 浮点数 | 0.0 | 在本地训练时，慢客户端（straggler）的比率。 |
| `--send_slow_rate` | `-ssr` | 浮点数 | 0.0 | 在向服务器发送模型更新时，慢客户端的比率。 |
| `--time_select`| `-ts`| 布尔值 | False | 如果为 True，每轮会根据客户端的耗时进行分组和选择。 |
| `--time_threthold` | `-tth`| 浮点数 | 10000 | 用于剔除慢客户端的时间阈值。 |

### 特定算法参数

#### 用于 pFedMe / PerAvg / FedProx / FedAMP / FedPHP / GPFL / FedCAC

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--beta` | `-bt` | 浮点数 | 0.0 | 用于这些算法的超参数。 |
| `--lamda` | `-lam` | 浮点数 | 1.0 | 正则化项的权重。 |
| `--mu` | `-mu` | 浮点数 | 0.0 | 用于这些算法的超参数。 |
| `--K` | `-K` | 整数 | 5 | pFedMe算法中个性化训练的步数。 |
| `--p_learning_rate` | `-lrp` | 浮点数 | 0.01 | pFedMe算法中用于个性化模型更新的学习率。 |

#### 用于 FedFomo

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--M` | `-M` | 整数 | 5 | 服务器在每一轮只发送 M 个客户端模型给某一个客户端。 |

#### 用于 FedMTL

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--itk` | `-itk` | 整数 | 4000 | 用于解决二次子问题的迭代次数。 |

#### 用于 FedAMP

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--alphaK` | `-alk` | 浮点数 | 1.0 | 论文中定义的超参数，通常为 lambda/sqrt(全局迭代次数)。 |
| `--sigma` | `-sg` | 浮点数 | 1.0 | FedAMP 算法的超参数。 |

#### 用于 APFL

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--alpha` | `-al` | 浮点数 | 1.0 | APFL 算法的超参数。 |

#### 用于 Ditto / FedRep

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--plocal_epochs`| `-pls`| 整数 | 1 | 个性化本地训练的周期数。 |

#### 用于 MOON / FedCAC / FedLC

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--tau` | `-tau` | 浮点数 | 1.0 | 用于这些算法的超参数。 |

#### 用于 FedBABU

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--fine_tuning_epochs`| `-fte`| 整数 | 10 | 模型微调的周期数。 |

#### 用于 APPLE

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--dr_learning_rate` | `-dlr` | 浮点数 | 0.0 | APPLE 算法的学习率。 |
| `--L` | `-L` | 浮点数 | 1.0 | APPLE 算法的超参数。 |

#### 用于 FedGen

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--noise_dim` | `-nd` | 整数 | 512 | 生成器网络中噪声向量的维度。 |
| `--generator_learning_rate`| `-glr`| 浮点数 | 0.005 | 生成器网络的学习率。 |
| `--hidden_dim` | `-hd` | 整数 | 512 | 隐藏层的维度。 |
| `--server_epochs` | `-se` | 整数 | 1000 | 服务器端训练的周期数。 |
| `--localize_feature_extractor` | `-lf`| 布尔值 | False | 如果为 True，特征提取器将被本地化。 |

#### 用于 SCAFFOLD / FedGH

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--server_learning_rate` | `-slr` | 浮点数 | 1.0 | 服务器端的学习率。 |

#### 用于 FedALA

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--eta` | `-et` | 浮点数 | 1.0 | FedALA 算法的超参数。 |
| `--rand_percent`| `-s` | 整数 | 80 | 随机百分比。 |
| `--layer_idx` | `-p` | 整数 | 2 | 用于更细粒度控制的层索引。 |

#### 用于 FedKD

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--mentee_learning_rate`| `-mlr`| 浮点数 | 0.005 | 学生模型（mentee）的学习率。 |
| `--T_start` | `-Ts` | 浮点数 | 0.95 | 知识蒸馏的起始温度系数。 |
| `--T_end` | `-Te` | 浮点数 | 0.98 | 知识蒸馏的结束温度系数。 |

#### 用于 FedDBE

| 参数 | 简写 | 类型 | 默认值 | 描述 |
|---|---|---|---|---|
| `--momentum` | `-mo` | 浮点数 | 0.1 | FedDBE 算法的动量（momentum）参数。 |
| `--kl_weight` | `-klw` | 浮点数 | 0.0 | KL 散度项的权重。 |